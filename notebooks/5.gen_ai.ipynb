{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inteligencia Artificial para generación en 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/griverat/Meteo-AI/blob/main/notebooks/5.gen_ai.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Si usa Google Colab, asegúrese de tener habilitada la GPU para este notebook.**\n",
    "\n",
    "![gpu_colab](../images/colab_gpu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción\n",
    "\n",
    "En este notebook se presenta un ejemplo de cómo se puede utilizar una red neuronal convolucional para generar datos en 2D. Se explorará la generación de variables climáticas en 2D con datos de entrada en 2D. \n",
    "\n",
    "## Objetivos\n",
    "\n",
    "- Comprender los conceptos básicos de las redes generativas.\n",
    "- Explorar las distintas arquitecturas de redes neuronales convolucionales para la generación de campos en 2D.\n",
    "- Desarrollar modelos de pronóstico con datos climáticos en 2D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generación de datos en 2D\n",
    "\n",
    "Previamente hemos explorado como una red neuronal puede tomar entradas tanto en 1D como en 2D para realizar predicciones en la forma de series temporales o clasificación. Estos casos presentaban una capa densa al final de la red para realizar la predicción. En este caso, la diferencia radica en que la red generativa no tiene una capa densa al final, sino que se utiliza una capa convolucional para generar la salida.\n",
    "\n",
    "Los principales tipos de redes generativas son:\n",
    "\n",
    "- **Generative Adversarial Networks (GANs)**: Son un tipo de red neuronal que se utiliza para generar datos nuevos. Consiste en dos redes neuronales, un generador y un discriminador, que compiten entre sí. El generador crea datos falsos y el discriminador intenta distinguir entre los datos reales y los falsos. El generador intenta engañar al discriminador y el discriminador intenta no ser engañado. Este proceso se repite hasta que el generador es capaz de generar datos que son indistinguibles de los datos reales.\n",
    "\n",
    "- **Variational Autoencoders (VAEs)**: Son un tipo de red neuronal que se utiliza para generar datos nuevos. Consiste en dos redes neuronales, un codificador y un decodificador. El codificador toma una entrada y la convierte en un vector latente. El decodificador toma el vector latente y lo convierte en una salida. El objetivo es que el decodificador sea capaz de generar datos que sean indistinguibles de los datos reales.\n",
    "\n",
    "En este notebook vamos a explorar algunos de los conceptos bases usados por estas arquitecturas ya que la finalidad es usar datos en 2D para generar datos en 2D. Las arquitecturas mencionadas anteriormente usualmente generan datos a partir de un vector latente que codifica la información de los datos de entrada, ya sea aleatorio o no.\n",
    "\n",
    "\n",
    "### Importar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import xarray as xr\n",
    "from IPython import display\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"monospace\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haremos uso del datos de reanálisis NCEP-NCAR 1 disponibles en la siguiente página web: https://psl.noaa.gov/data/gridded/data.ncep.reanalysis.html\n",
    "El proyecto NCEP/NCAR Reanalysis 1 utiliza un sistema de análisis/pronóstico de última generación para realizar la asimilación de datos utilizando datos pasados ​​desde 1948 hasta el presente.\n",
    "\n",
    "Para este ejemplo, utilizaremos los datos de temperatura y presión en superficie como predictores, y la precipitación como variable a predecir. Los datos que vamos a leer han sido cargados y recortados para una región específica sobre Perú con la finalidad de facilitar su uso en este notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta celda contiene el codigo usado para el recorte y preprocesamiento de los datos\n",
    "# puede ser reusado para la región de interés luego de descargar los datos de la\n",
    "# pagina de NCEP\n",
    "# temp = (\n",
    "#     xr.open_mfdataset(\"/Users/dangomelon/Downloads/skin_temperature/*.nc\")\n",
    "#     .sel(time=slice(\"2020\", None))\n",
    "#     .skt.load()\n",
    "#     - 273.15\n",
    "# )\n",
    "# temp = temp.sortby(\"lat\").sel(lat=slice(-20, 10), lon=slice(278, 308))\n",
    "# temp.to_netcdf(\"data/ncep_temp.nc\")\n",
    "# precip = (\n",
    "#     xr.open_mfdataset(\"/Users/dangomelon/Downloads/prate/*.nc\")\n",
    "#     .sel(time=slice(\"2020\", None))\n",
    "#     .prate.load()\n",
    "#     * 3600\n",
    "#     * 24\n",
    "# )\n",
    "# precip = precip.sortby(\"lat\").sel(lat=slice(-20, 10), lon=slice(278, 308))\n",
    "# precip.to_netcdf(\"data/ncep_precip.nc\")\n",
    "# pres = (\n",
    "#     xr.open_mfdataset(\"/Users/dangomelon/Downloads/pres/*.nc\")\n",
    "#     .sel(time=slice(\"2020\", None))\n",
    "#     .pres.load()\n",
    "# )\n",
    "# pres = pres.sortby(\"lat\").sel(lat=slice(-20, 10), lon=slice(278, 308))\n",
    "# pres.to_netcdf(\"data/ncep_pres.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!wget https://raw.githubusercontent.com/griverat/Meteo-AI/main/notebooks/data/ncep_temp.nc -O data/ncep_temp.nc\n",
    "!wget https://raw.githubusercontent.com/griverat/Meteo-AI/main/notebooks/data/ncep_precip.nc -O data/ncep_precip.nc\n",
    "!wget https://raw.githubusercontent.com/griverat/Meteo-AI/main/notebooks/data/ncep_pres.nc -O data/ncep_pres.nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = xr.open_dataarray(\"data/ncep_temp.nc\")\n",
    "precip = xr.open_dataarray(\"data/ncep_precip.nc\")\n",
    "pres = xr.open_dataarray(\"data/ncep_pres.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploramos el último dato de nuestras variables para tener una idea de cómo se ven los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(\n",
    "    1,\n",
    "    3,\n",
    "    figsize=(15, 5),\n",
    "    subplot_kw={\"projection\": ccrs.PlateCarree(central_longitude=180)},\n",
    ")\n",
    "axs = axs.flat\n",
    "for i, (da, title) in enumerate(\n",
    "    zip(\n",
    "        [temp, pres, precip],\n",
    "        [\"Temperature\", \"Pressure\", \"Precipitation\"],\n",
    "    )\n",
    "):\n",
    "    ax = axs[i]\n",
    "    da.isel(time=-1).plot(\n",
    "        ax=ax, transform=ccrs.PlateCarree(), cmap=\"viridis\", x=\"lon\", y=\"lat\"\n",
    "    )\n",
    "    ax.coastlines()\n",
    "    ax.set_title(title)\n",
    "    gl = ax.gridlines(draw_labels=True, ls=\"--\", lw=0.5)\n",
    "    gl.top_labels = False\n",
    "    gl.right_labels = False\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparácion de los datos\n",
    "\n",
    "Los datos tienen resolución temporal de 1 día, por lo que intentaremos usar los valores de los campos de temperatura y presión en superficie para predecir la precipitación en el siguiente día. Cabe notar que la elección de las variables es extremadamente importante al momento de diseñar un modelo de pronóstico, ya que no todas las variables son útiles para predecir una variable en particular. En este caso en particular, puede que la combinación de estas variables no sea la mejor para predecir la precipitación, pero su bajo peso y disponibilidad de datos en un solo nivel de presión facilita su uso en este ejemplo. En caso se quiera implementar una red con mejor precisión, se recomienda usar más variables en distintos niveles de presión.\n",
    "\n",
    "Vamos a desfasar los datos de precipitation en 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_shifted = precip.shift(time=-1).dropna(\"time\")\n",
    "temp_data = temp.sel(time=precip_shifted.time)\n",
    "pres_data = pres.sel(time=precip_shifted.time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora realizamos la separacion de los datos en conjuntos de entrenamiento, validación y prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(temp_data))\n",
    "val_size = int(0.1 * len(temp_data))\n",
    "test_size = len(temp_data) - train_size - val_size\n",
    "print(f\"Train size: {train_size}, Val size: {val_size}, Test size: {test_size}\")\n",
    "\n",
    "temp_train, temp_val, temp_test = (\n",
    "    temp_data[:train_size],\n",
    "    temp_data[train_size : train_size + val_size],\n",
    "    temp_data[train_size + val_size :],\n",
    ")\n",
    "pres_train, pres_val, pres_test = (\n",
    "    pres_data[:train_size],\n",
    "    pres_data[train_size : train_size + val_size],\n",
    "    pres_data[train_size + val_size :],\n",
    ")\n",
    "\n",
    "precip_train, precip_val, precip_test = (\n",
    "    precip_shifted[:train_size],\n",
    "    precip_shifted[train_size : train_size + val_size],\n",
    "    precip_shifted[train_size + val_size :],\n",
    ")\n",
    "\n",
    "train_data = xr.concat([temp_train, pres_train], dim=\"channel\").transpose(\n",
    "    \"time\", \"lat\", \"lon\", \"channel\"\n",
    ")\n",
    "\n",
    "val_data = xr.concat([temp_val, pres_val], dim=\"channel\").transpose(\n",
    "    \"time\", \"lat\", \"lon\", \"channel\"\n",
    ")\n",
    "\n",
    "test_data = xr.concat([temp_test, pres_test], dim=\"channel\").transpose(\n",
    "    \"time\", \"lat\", \"lon\", \"channel\"\n",
    ")\n",
    "\n",
    "mean = train_data.mean((\"time\", \"lat\", \"lon\"))\n",
    "std = train_data.std((\"time\", \"lat\", \"lon\"))\n",
    "\n",
    "train_data = (train_data - mean) / std\n",
    "val_data = (val_data - mean) / std\n",
    "test_data = (test_data - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cabe resaltar que nuestros datos de entrada estan compuestos por 2 variables en 2D, por lo que las variables fueron combinadas en un solo tensor de entrada con la forma `(time, lat, lon, 2)`. De igual manera, se estandarizaron los datos de entrada debido al uso de variables con distintas escalas.\n",
    "\n",
    "Haremos uso de la interfaz `Dataset` de tensorflow para cargar los datos con facilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.zip(\n",
    "    (\n",
    "        tf.data.Dataset.from_tensor_slices(train_data.values),\n",
    "        tf.data.Dataset.from_tensor_slices(precip_train.values),\n",
    "    )\n",
    ")\n",
    "\n",
    "val_data = tf.data.Dataset.zip(\n",
    "    (\n",
    "        tf.data.Dataset.from_tensor_slices(val_data.values),\n",
    "        tf.data.Dataset.from_tensor_slices(precip_val.values),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos los datos organizados, vamos a comenzar definiendo una red neuronal convolucional como nuestra linea base para la generación de datos en 2D.\n",
    "\n",
    "\n",
    "### Definición del modelo FCN\n",
    "\n",
    "La red neuronal estara compuesta por una serie de capas convolucionales que mantienen la forma de nuestros datos de entrada y solo varían en la cantidad de filtros usados en cada capa. La salida de la red será un tensor de la misma forma que nuestros datos de entrada, pero con un solo canal de salida. Este tipo de red toma ventaja de la capacidad de las capas convolucionales para aprender patrones en 2D y generar datos en 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fcn_model(input_shape):\n",
    "    model_input = tf.keras.layers.Input(shape=input_shape)\n",
    "    x = tf.keras.layers.Conv2D(32, 3, activation=\"relu\", padding=\"same\")(model_input)\n",
    "    x = tf.keras.layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = tf.keras.layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = tf.keras.layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = tf.keras.layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = tf.keras.layers.Conv2D(32, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "\n",
    "    model_output = tf.keras.layers.Conv2D(1, 3, activation=\"linear\", padding=\"same\")(x)\n",
    "\n",
    "    # ya que la salida es precipitación, no puede ser negativa\n",
    "    model_output = tf.keras.layers.ReLU()(model_output)\n",
    "\n",
    "    model = tf.keras.Model(model_input, model_output)\n",
    "    return model\n",
    "\n",
    "\n",
    "fcn_model = get_fcn_model(tuple(train_data.element_spec[0].shape))\n",
    "fcn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede intentar variar algunos de los hiperparámetros de la red para ver si se obtienen mejores resultados. Por ejemplo, se puede intentar aumentar la cantidad de filtros en cada capa, o aumentar la cantidad de capas convolucionales. También se puede intentar modificar la cantidad de épocas de entrenamiento o el tamaño del lote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "fcn_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=10, restore_best_weights=True, start_from_epoch=10\n",
    "    )\n",
    "]\n",
    "\n",
    "history = fcn_model.fit(\n",
    "    train_data.shuffle(len(train_data)).batch(batch_size),\n",
    "    validation_data=val_data.batch(batch_size),\n",
    "    epochs=100,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisamos las métricas de la red para ver cómo se comporta en el conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame(history.history)\n",
    "metrics.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora evaluamos la red con el set de pruebas. Nuestros resultados pueden variar dependiendo de la cantidad de datos de entrenamiento y la cantidad de épocas que el modelo pudo entrenar antes de que el algoritmo de EarlyStopping detenga el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = fcn_model.evaluate(test_data, precip_test)\n",
    "print(f\"Test MSE: {test_metrics:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colocamos la salida en un objeto de `xarray` que sea mas facil de manejar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = fcn_model.predict(test_data)\n",
    "preds = xr.DataArray(\n",
    "    preds.squeeze(),\n",
    "    dims=[\"time\", \"lat\", \"lon\"],\n",
    "    coords={\n",
    "        \"time\": test_data.time,\n",
    "        \"lat\": test_data.lat,\n",
    "        \"lon\": test_data.lon,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos un gráfico con 3 paneles para comparar los datos reales con los datos generados y el error entre ellos. De esta forma podemos ver en que ubicaciones la red ha sobreestimado o subestimado la precipitación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(0, len(test_data))\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    1,\n",
    "    3,\n",
    "    figsize=(15, 5),\n",
    "    subplot_kw={\"projection\": ccrs.PlateCarree(central_longitude=180)},\n",
    ")\n",
    "axs = axs.flat\n",
    "precip_test.isel(time=idx).plot(\n",
    "    ax=axs[0], transform=ccrs.PlateCarree(), cmap=\"viridis\", vmax=20\n",
    ")\n",
    "preds.isel(time=idx).plot(\n",
    "    ax=axs[1], transform=ccrs.PlateCarree(), cmap=\"viridis\", vmax=20\n",
    ")\n",
    "(preds - precip_test).isel(time=idx).plot(\n",
    "    ax=axs[2],\n",
    "    transform=ccrs.PlateCarree(),\n",
    "    cmap=\"coolwarm\",\n",
    "    robust=True,\n",
    "    vmax=10,\n",
    "    vmin=-10,\n",
    ")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.coastlines()\n",
    "    gl = ax.gridlines(draw_labels=True, ls=\"--\", lw=0.5, alpha=0.5)\n",
    "    gl.top_labels = False\n",
    "    gl.right_labels = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitectura U-Net\n",
    "\n",
    "La arquitectura U-Net es una red neuronal convolucional que se utiliza para la segmentación de imágenes. La red toma una imagen de entrada y produce una imagen de salida que contiene la segmentación de la imagen de entrada. La red consta de dos partes, un codificador y un decodificador. El codificador toma la imagen de entrada y la reduce a una representación latente. El decodificador toma la representación latente y la expande a la imagen de salida. La red utiliza conexiones residuales para ayudar a la propagación del gradiente durante el entrenamiento.\n",
    "\n",
    "A diferenca de la red FCN, la red U-Net tiene una arquitectura más compleja que permite aprender patrones en distintas escalas de la imagen. La red U-Net es una red profunda que consta de varias capas convolucionales y de pooling en el codificador, y varias capas convolucionales y de upsampling en el decodificador. Usualmente hacen uso de conexiones residuales para ayudar a la propagación del gradiente durante el entrenamiento.\n",
    "\n",
    "En este ejemplo vamos a implementar una red U-Net para la generación de datos en 2D sin conexiones residuales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unet_model(input_shape=(16, 16, 1)):\n",
    "    model_input = tf.keras.layers.Input(shape=input_shape)\n",
    "    x = tf.keras.layers.Conv2D(32, 3, activation=\"relu\", padding=\"same\")(model_input)\n",
    "    x = tf.keras.layers.Conv2D(32, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = tf.keras.layers.MaxPool2D()(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = tf.keras.layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = tf.keras.layers.MaxPool2D()(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = tf.keras.layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = tf.keras.layers.MaxPool2D()(x)\n",
    "\n",
    "    bottleneck = tf.keras.layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2DTranspose(\n",
    "        128, 3, strides=2, activation=\"relu\", padding=\"same\"\n",
    "    )(bottleneck)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    x = tf.keras.layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = tf.keras.layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2DTranspose(\n",
    "        64, 3, strides=2, activation=\"relu\", padding=\"same\"\n",
    "    )(x)\n",
    "    x = tf.keras.layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = tf.keras.layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2DTranspose(\n",
    "        32, 3, strides=2, activation=\"relu\", padding=\"same\"\n",
    "    )(x)\n",
    "    x = tf.keras.layers.Conv2D(32, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "\n",
    "    model_output = tf.keras.layers.Conv2D(1, 3, activation=\"linear\", padding=\"same\")(x)\n",
    "\n",
    "    # ya que la salida es precipitación, no puede ser negativa\n",
    "    model_output = tf.keras.layers.ReLU()(model_output)\n",
    "\n",
    "    model = tf.keras.Model(model_input, model_output)\n",
    "    return model\n",
    "\n",
    "\n",
    "unet_model = get_unet_model(tuple(train_data.element_spec[0].shape))\n",
    "unet_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora seguimos con el código para el entrenamiento de la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "unet_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=10, restore_best_weights=True, start_from_epoch=10\n",
    "    )\n",
    "]\n",
    "\n",
    "history = unet_model.fit(\n",
    "    train_data.shuffle(len(train_data)).batch(batch_size),\n",
    "    validation_data=val_data.batch(batch_size),\n",
    "    epochs=100,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuevamente, esto nos puede generar resultados variados, todo depende de la inicialización aleatoria de los pesos de la red. En caso de que los resultados no sean los esperados, se puede intentar cambiar algunos hiperparámetros de la red, como la cantidad de filtros en cada capa, la cantidad de capas convolucionales, la cantidad de épocas de entrenamiento, o el tamaño del lote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame(history.history)\n",
    "metrics.plot(ylim=(0, 150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = unet_model.predict(test_data)\n",
    "preds = xr.DataArray(\n",
    "    preds.squeeze(),\n",
    "    dims=[\"time\", \"lat\", \"lon\"],\n",
    "    coords={\n",
    "        \"time\": test_data.time,\n",
    "        \"lat\": test_data.lat,\n",
    "        \"lon\": test_data.lon,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(0, len(test_data))\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    1,\n",
    "    3,\n",
    "    figsize=(15, 5),\n",
    "    subplot_kw={\"projection\": ccrs.PlateCarree(central_longitude=180)},\n",
    ")\n",
    "axs = axs.flat\n",
    "precip_test.isel(time=idx).plot(\n",
    "    ax=axs[0], transform=ccrs.PlateCarree(), cmap=\"viridis\", vmax=20\n",
    ")\n",
    "preds.isel(time=idx).plot(\n",
    "    ax=axs[1], transform=ccrs.PlateCarree(), cmap=\"viridis\", vmax=20\n",
    ")\n",
    "(preds - precip_test).isel(time=idx).plot(\n",
    "    ax=axs[2],\n",
    "    transform=ccrs.PlateCarree(),\n",
    "    cmap=\"coolwarm\",\n",
    "    robust=True,\n",
    "    vmax=10,\n",
    "    vmin=-10,\n",
    ")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.coastlines()\n",
    "    gl = ax.gridlines(draw_labels=True, ls=\"--\", lw=0.5, alpha=0.5)\n",
    "    gl.top_labels = False\n",
    "    gl.right_labels = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-Net con conexiones residuales\n",
    "\n",
    "La red U-Net con conexiones residuales es una variante de la red U-Net que ayuda a la propagación del gradiente durante el entrenamiento. La red consta de dos partes, un codificador y un decodificador. El codificador toma la imagen de entrada y la reduce a una representación latente. El decodificador toma la representación latente y la expande a la imagen de salida. Las conexiones residuales conectan las capas del codificador con las capas del decodificador para ayudar de esta forma a la propagación del gradiente y mejorar el rendimiento de la red.\n",
    "\n",
    "Ya que esta red se compone de varias secuencias de capas convolucionales y de pooling en el codificador, y capas convolucionales y de upsampling en el decodificador, usaremos funciones que nos ayuden a definir estas capas de forma más sencilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_conv_block(x, n_filters):\n",
    "\n",
    "    # Conv2D luego ReLU\n",
    "    x = tf.keras.layers.Conv2D(n_filters, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    # Conv2D luego ReLU\n",
    "    x = tf.keras.layers.Conv2D(n_filters, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def downsample_block(x, n_filters):\n",
    "    f = double_conv_block(x, n_filters)\n",
    "    p = tf.keras.layers.MaxPool2D(2)(f)\n",
    "    p = tf.keras.layers.Dropout(0.3)(p)\n",
    "\n",
    "    return f, p\n",
    "\n",
    "\n",
    "def upsample_block(x, conv_features, n_filters):\n",
    "    # upsample\n",
    "    x = tf.keras.layers.Conv2DTranspose(n_filters, 3, 2, padding=\"same\")(x)\n",
    "    # concatenar con la salida de la capa de downsample\n",
    "    # aca es donde se realiza la conexion residual\n",
    "    x = tf.keras.layers.concatenate([x, conv_features])\n",
    "    # dropout\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    # Conv2D luego ReLU\n",
    "    x = double_conv_block(x, n_filters)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def unet_res():\n",
    "    # entrada\n",
    "    inputs = tf.keras.layers.Input(shape=(16, 16, 2))\n",
    "\n",
    "    # encoder: camino de reducción - downsample\n",
    "    # 1 - downsample\n",
    "    f1, p1 = downsample_block(inputs, 64)\n",
    "    # 2 - downsample\n",
    "    f2, p2 = downsample_block(p1, 128)\n",
    "    # 3 - downsample\n",
    "    f3, p3 = downsample_block(p2, 256)\n",
    "    # 4 - downsample\n",
    "    f4, p4 = downsample_block(p3, 512)\n",
    "\n",
    "    # 5 - bottleneck - cuello de botella - espacio latente\n",
    "    bottleneck = double_conv_block(p4, 1024)\n",
    "\n",
    "    # decoder: camino de expansión - upsample\n",
    "    # 6 - upsample\n",
    "    u6 = upsample_block(bottleneck, f4, 512)\n",
    "    # 7 - upsample\n",
    "    u7 = upsample_block(u6, f3, 256)\n",
    "    # 8 - upsample\n",
    "    u8 = upsample_block(u7, f2, 128)\n",
    "    # 9 - upsample\n",
    "    u9 = upsample_block(u8, f1, 64)\n",
    "\n",
    "    # salidas\n",
    "    outputs = tf.keras.layers.Conv2D(1, 1, padding=\"same\", activation=\"relu\")(u9)\n",
    "\n",
    "    unet_model = tf.keras.Model(inputs, outputs, name=\"U-Net\")\n",
    "\n",
    "    return unet_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a explorar la arquitectura de la red U-Net con conexiones residuales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = unet_res()\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provee una forma gráfica de visualizar la arquitectura de la red. Esto nos puede ayudar a entender mejor cómo se conectan las capas de la red y cómo se propagan los datos a través de la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(generator, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a la complejidad del modelo, el entrenamiento puede tardar más tiempo que los modelos anteriores. Se recomienda tener paciencia y dejar que el modelo se entrene por un número suficiente de épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "generator.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=10, restore_best_weights=True, start_from_epoch=10\n",
    "    )\n",
    "]\n",
    "\n",
    "history = generator.fit(\n",
    "    train_data.shuffle(len(train_data)).batch(batch_size),\n",
    "    validation_data=val_data.batch(batch_size),\n",
    "    epochs=100,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame(history.history)\n",
    "metrics.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = generator.predict(test_data)\n",
    "preds = xr.DataArray(\n",
    "    preds.squeeze(),\n",
    "    dims=[\"time\", \"lat\", \"lon\"],\n",
    "    coords={\n",
    "        \"time\": test_data.time,\n",
    "        \"lat\": test_data.lat,\n",
    "        \"lon\": test_data.lon,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(0, len(test_data))\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    1,\n",
    "    3,\n",
    "    figsize=(15, 5),\n",
    "    subplot_kw={\"projection\": ccrs.PlateCarree(central_longitude=180)},\n",
    ")\n",
    "axs = axs.flat\n",
    "precip_test.isel(time=idx).plot(\n",
    "    ax=axs[0], transform=ccrs.PlateCarree(), cmap=\"viridis\", vmax=20\n",
    ")\n",
    "preds.isel(time=idx).plot(\n",
    "    ax=axs[1], transform=ccrs.PlateCarree(), cmap=\"viridis\", vmax=20\n",
    ")\n",
    "(preds - precip_test).isel(time=idx).plot(\n",
    "    ax=axs[2],\n",
    "    transform=ccrs.PlateCarree(),\n",
    "    cmap=\"coolwarm\",\n",
    "    robust=True,\n",
    "    vmax=10,\n",
    "    vmin=-10,\n",
    ")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.coastlines()\n",
    "    gl = ax.gridlines(draw_labels=True, ls=\"--\", lw=0.5, alpha=0.5)\n",
    "    gl.top_labels = False\n",
    "    gl.right_labels = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meteo_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
