{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning con redes convolucionales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/griverat/Meteo-AI/blob/main/notebooks/2.cnn.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción\n",
    "\n",
    "Este notebook contiene el material a desarrollar durante la sesión de Deep Learning. Se presentarán los conceptos básicos de deep learning y como es que se diferencia principalmente de las redes neuronales tradicionales. Se presentarán ejemplos de aplicaciones y se realizará un ejercicio práctico de clasificación de imágenes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos\n",
    "\n",
    "- Conocer los conceptos básicos de deep learning.\n",
    "- Entender cuando es conveniente utilizar deep learning.\n",
    "- Implementar un modelo de deep learning para clasificación de imágenes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sobre el Aprendizaje Profundo\n",
    "\n",
    "A diferencia de las redes neuronales tradicionales, las redes neuronales profundas (deep learning) son capaces de aprender representaciones jerárquicas de los datos. Esto significa que las redes neuronales profundas pueden aprender a representar los datos en diferentes niveles de abstracción. Por ejemplo, en el caso de imágenes, una red neuronal profunda puede aprender a representar los datos en términos de bordes, formas, texturas, etc.\n",
    "\n",
    "Debido a la complejidad de las redes neuronales profundas, el perceptrón multicapa (MLP) dejó de ser suficiente para resolver problemas complejos ya que requiere de una gran cantidad de parámetros y por ende una gran cantidad de poder computacional. Como solución a este problema, se desarrollaron las redes neuronales convolucionales (CNN) que son capaces de aprender representaciones jerárquicas de los datos de manera más eficiente.\n",
    "\n",
    "## Redes Neuronales Convolucionales\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/6/63/Typical_cnn.png\"></img>\n",
    "\n",
    "Arquitectura convolucional típica.\n",
    "By <a href=\"//commons.wikimedia.org/w/index.php?title=User:Aphex34&amp;action=edit&amp;redlink=1\" class=\"new\" title=\"User:Aphex34 (page does not exist)\">Aphex34</a> - <span class=\"int-own-work\" lang=\"en\">Own work</span>, <a href=\"https://creativecommons.org/licenses/by-sa/4.0\" title=\"Creative Commons Attribution-Share Alike 4.0\">CC BY-SA 4.0</a>, <a href=\"https://commons.wikimedia.org/w/index.php?curid=45679374\">Link</a>\n",
    "\n",
    "Una red neuronal convolucional (CNN) es una red neuronal que contiene una o más capas convolucionales. Las capas convolucionales son capaces de aprender representaciones jerárquicas de los datos de manera más eficiente que las redes neuronales tradicionales.\n",
    "\n",
    "Los bloques básicos de una CNN son:\n",
    "\n",
    "- **Capa convolucional**: Esta capa aplica una serie de filtros a la entrada y produce un mapa de características. Cada filtro es capaz de aprender diferentes características de los datos.\n",
    "- **Capa de activación**: Esta capa aplica una función de activación a la salida de la capa convolucional. La función de activación más común es la función ReLU.\n",
    "- **Capa de agrupación**: Esta capa reduce la dimensionalidad de la entrada aplicando una operación de agrupación. La operación de agrupación más común es la operación de agrupación máxima.\n",
    "- **Capa completamente conectada**: Esta capa toma la salida de las capas anteriores y produce la salida final de la red.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import xarray as xr\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"monospace\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura de datos\n",
    "\n",
    "Como introducción a las redes neuronales convolucionales, utilizaremos el conjunto de datos MNIST para familiarizarnos con el proceso de entrenamiento de una red neuronal convolucional. El conjunto de datos MNIST contiene imágenes de dígitos escritos a mano y es uno de los conjuntos de datos más utilizados en el campo del aprendizaje automático."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previsualizamos la data descargada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 10, figsize=(10, 1))\n",
    "for i in range(10):\n",
    "    ax[i].imshow(x_train[i], cmap=\"gray\")\n",
    "    ax[i].axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como estamos tratando con imágenes, debemos normalizar los datos para que se encuentren en un rango de 0 a 1. Para ello, dividimos los valores de los píxeles por 255, que es el valor máximo de un píxel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(-1, 28, 28, 1).astype(np.float32) / 255.0\n",
    "x_test = x_test.reshape(-1, 28, 28, 1).astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos verificar cuales son las dimensiones de nuestros datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que hicimos en este caso fue asegurarnos que la imagen tenga al menos una dimensión de color ademas de la altura y el ancho. En este caso, la imagen tiene una dimensión de color, 28 píxeles de alto y 28 píxeles de ancho. A esta dimension de color se le llama canal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que la data de entrada se encuentra lista, nos enfocamos en las etiquetas o datos de salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos verificar que se trata de números enteros que van del 0 al 9, correspondientes a los dígitos escritos a mano. Para poder utilizar estos datos en un modelo de clasificación, necesitamos convertirlos a un formato de one-hot encoding.\n",
    "\n",
    "**¿Qué es one-hot encoding?**\n",
    "\n",
    "One-hot encoding es una técnica utilizada en el aprendizaje automático para representar datos categóricos como vectores binarios. En el caso de las etiquetas de los dígitos, cada etiqueta se convierte en un vector binario de 10 elementos, donde el elemento correspondiente a la etiqueta es 1 y los demás elementos son 0.\n",
    "\n",
    "<img src=\"https://th.bing.com/th/id/OIP.cnmpSdK-6hAQJdTUBFxcnAHaB3?rs=1&pid=ImgDetMain\"></src>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora verificamos que las etiquetas se encuentren en el formato correcto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear el modelo\n",
    "\n",
    "Para crear un modelo de red neuronal convolucional, utilizamos la clase Sequential de Keras. La clase Sequential nos permite crear un modelo de red neuronal apilando capas una encima de la otra.\n",
    "\n",
    "El primer paso para crear un modelo de red neuronal convolucional es definir la arquitectura del modelo. En este caso, utilizaremos una arquitectura simple con dos capas convolucionales seguidas cada una por una capa de agrupación y finalmente una capa completamente conectada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Conv2D(16, (3, 3), activation=\"relu\"),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "input_shape = (None, 28, 28, 1)\n",
    "model.build(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compilar y entrenar el modelo\n",
    "\n",
    "Una vez que hemos definido la arquitectura del modelo, debemos compilarlo y entrenarlo. Para compilar el modelo, utilizamos el método compile y especificamos la función de pérdida, el optimizador y las métricas que queremos utilizar para evaluar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 15\n",
    "\n",
    "history = model.fit(\n",
    "    x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos visualizar la función de perdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "ax[0].plot(history.history[\"accuracy\"], label=\"train\")\n",
    "ax[0].plot(history.history[\"val_accuracy\"], label=\"validation\")\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Accuracy\")\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(history.history[\"loss\"], label=\"train\")\n",
    "ax[1].plot(history.history[\"val_loss\"], label=\"validation\")\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].set_ylabel(\"Loss\")\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluar el modelo\n",
    "\n",
    "Una vez que hemos entrenado el modelo, podemos evaluar su rendimiento en un conjunto de datos de prueba. Para evaluar el modelo, utilizamos el método evaluate y pasamos el conjunto de datos de prueba y las etiquetas de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicciones\n",
    "\n",
    "Finalmente, podemos utilizar el modelo entrenado para hacer predicciones sobre nuevos datos. Para hacer predicciones, utilizamos el método predict y pasamos los datos de entrada sobre los que queremos hacer predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = np.random.choice(len(x_test), 25)\n",
    "images = x_test[idxs]\n",
    "\n",
    "preds = model.predict(images).argmax(-1)\n",
    "\n",
    "fig, ax = plt.subplots(5, 5, figsize=(10, 10))\n",
    "for i in range(25):\n",
    "    ax[i // 5, i % 5].imshow(images[i].reshape(28, 28), cmap=\"gray\")\n",
    "    ax[i // 5, i % 5].axis(\"off\")\n",
    "    ax[i // 5, i % 5].set_title(\n",
    "        f\"Predicted: {preds[i]}\\nTrue: {y_test[idxs[i]].argmax()}\"\n",
    "    )\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, el modelo es capaz de predecir en un 98% (aproximadamente) de los casos el dígito correcto.\n",
    "Este es un ejemplo básico de cómo se puede utilizar una red neuronal convolucional para clasificar imágenes.\n",
    "La base de datos de digitos escritos a mano junto con las etiquetas ya se encontraban en un formato adecuado para ser utilizadas en un modelo de clasificación, requiriendo únicamente de una normalización y un one-hot encoding. En otros casos, usualmente se requiere de un preprocesamiento más complejo de los datos antes de ser utilizados en un modelo de clasificación, como lo vamos a ver en el siguiente ejemplo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pronóstico de la fase de ENOS en el Pacífico Central\n",
    "\n",
    "En este ejemplo, utilizaremos una red neuronal convolucional para pronosticar la fase de El Niño La Oscilacion Sur (ENOS) en el Pacífico Central. El fenómeno de El Niño es un fenómeno climático que se caracteriza por el calentamiento anómalo de las aguas del Océano Pacífico ecuatorial y que tiene un impacto significativo en el clima global.\n",
    "\n",
    "Esta demostración tomará como datos de entrada las anomalías de temperatura superficial del mar (SST) en la región del Pacífico durante los últimos 12 meses y pronosticará la fase de El Niño en los próximos 6 meses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a descargar los datos de temperatura superficial del mar de Extended Reconstructed Sea Surface Temperature version 5 (ERSSTv5), sobre la cual haremos el cálculo del índice EN3.4 para determinar la fase de ENOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# esta descarga podria tomar algo de tiempo\n",
    "!mkdir data\n",
    "!wget https://downloads.psl.noaa.gov/Datasets/noaa.ersst.v5/sst.mnmean.nc -O data/sst.mnmean.nc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizaremos `xarray` para leer los datos de temperatura superficial del mar y calcular el índice EN3.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst = xr.open_dataset(\"data/sst.mnmean.nc\").sst.sortby(\"lat\")\n",
    "sst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comenzar, los datos deben de estar en terminos de anomalías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clim_period = slice(\"1991\", \"2020\")\n",
    "sst_clim = sst.sel(time=clim_period).groupby(\"time.month\").mean(\"time\")\n",
    "sst_clim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "sst_clim.sel(month=1).plot(ax=ax[0], cmap=\"inferno\")\n",
    "ax[0].set_title(\"January\")\n",
    "sst_clim.sel(month=7).plot(ax=ax[1], cmap=\"inferno\")\n",
    "ax[1].set_title(\"July\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El cálculo de las anomalías es trivial usando los métodos de `xarray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_anom = sst.groupby(\"time.month\") - sst_clim\n",
    "sst_anom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora calculamos el índice EN3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en34 = sst_anom.sel(lat=slice(-5, 5), lon=slice(190, 240)).mean(dim=[\"lat\", \"lon\"])\n",
    "en34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos visualizar el índice EN3.4 para verificar que se haya calculado correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = en34.sel(time=slice(\"1980\", None))\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "subset.plot(ax=ax, color=\"k\", linewidth=0.5)\n",
    "ax.set_title(\"EN3.4 Index\")\n",
    "\n",
    "ax.fill_between(subset.time, subset, 0.5, where=subset > 0.5, color=\"red\", alpha=0.5)\n",
    "ax.fill_between(subset.time, subset, -0.5, where=subset < -0.5, color=\"blue\", alpha=0.5)\n",
    "\n",
    "ax.axhline(0.5, color=\"red\", linestyle=\"--\", lw=0.5)\n",
    "ax.axhline(-0.5, color=\"blue\", linestyle=\"--\", lw=0.5)\n",
    "ax.axhline(0, color=\"black\", linestyle=\"-\", lw=0.5)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`xarray` facilita el uso y manejo de datos climaticos gracias a las funciones de agregación y selección de datos.\n",
    "\n",
    "Ahora que tenemos las SSTA y el índice EN3.4, podemos proceder a ordenar los datos de acuerdo a la tarea que queremos realizar. primero debemos agrupar los datos en secuencias de 12 meses para las SSTA y 6 meses para el índice EN3.4.\n",
    "\n",
    "Para lograr esto, vamos a explorar como es que el método `rolling` de `xarray` nos puede ayudar a lograr esto.\n",
    "`rolling` nos permite crear una ventana movil sobre la cual podemos aplicar funciones de agregación. En nuestro caso, no estamos interesados en agregar los datos, sino en obtener las secuencias consecutivas.\n",
    "\n",
    "Crearemos un `DataArray` con numeros del 0 al 9 para poder visualizar como funciona el método `rolling`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = xr.DataArray(\n",
    "    np.arange(1, 13),\n",
    "    dims=[\"time\"],\n",
    "    coords={\"time\": pd.date_range(\"2020-01-01\", periods=12)},\n",
    ")\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.rolling(time=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para extraer los grupos, debemos construir una nueva dimension en donde sera almacenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.rolling(time=3).construct(\"window_dim\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`rolling` comienza desde la izquierda mientras alinea los datos hacia la derecha, llenando los valores faltantes con `NaN`. Si nos deshacemos de estos valores, obtendremos las secuencias que necesitamos.\n",
    "\n",
    "Si nos fijamos bien en los tiempos que han sido seleccionados, podemos ver que los dos primeros tiempos tienen valores faltantes y recien a partir del tercer valor se encuentran completos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.rolling(time=3).construct(\"window_dim\").isel(time=slice(2, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cabe observar que en este pequeño ejemplo, el mes 03 esta asociado al valor 3 que se encuentra al final de la primera fila, por lo que nuestras etiquetas en el tiempo representan el tiempo final de la secuencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.rolling(time=3).construct(\"window_dim\").isel(time=slice(2, None))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llevando este ejemplo al set de datos de SSTA con 12 meses, obtenemos lo siguiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_grouped = sst_anom.rolling(time=12).construct(\"channel\").isel(time=slice(11, None))\n",
    "sst_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_grouped[0].plot(col=\"channel\", col_wrap=4, cmap=\"coolwarm\", vmin=-3, vmax=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos de entrada ya se encuentran agrupados en secuencias de 12 meses, en donde la etiqueta de tiempo corresponde al último mes de la secuencia. Ahora debemos hacer lo mismo con el índice EN3.4. Cabe recordar que el objetivo es pronosticar la fase del ENOS en base al indice EN3.4 en los próximos 6 meses, por lo que primero debemos definir las clases. Por simplicidad, definiremos tres clases: El Niño, La Niña y Neutral, en base a los valores del índice EN3.4.\n",
    "\n",
    "- Clase 0 - Neutral: -0.5 <= EN3.4 <= 0.5\n",
    "- Clase 1 - El Niño: EN3.4 > 0.5\n",
    "- Clase 2 - La Niña: EN3.4 < -0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enso_class = xr.where(en34 > 0.5, 1, xr.where(en34 < -0.5, 2, 0))\n",
    "enso_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enso_class.tail(100).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora usaremos esta nueva variable de clase para agrupar los datos en secuencias de 6 meses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enso_class_grouped = (\n",
    "    enso_class.rolling(time=6).construct(\"channel\").isel(time=slice(5, None))\n",
    ")\n",
    "\n",
    "# ya que rolling asigna el tiempo al ultimo valor de la secuencia, lo corregimos restando 5 meses\n",
    "# de esta manera el tiempo se asigna al valor inicial de la secuencia\n",
    "enso_class_grouped[\"time\"] = pd.to_datetime(enso_class_grouped.time) - pd.DateOffset(\n",
    "    months=5\n",
    ")\n",
    "\n",
    "# finalmente, desplazamos la secuencia en un paso hacia adelante\n",
    "# de esta manera, el valor en el tiempo t, corresponde a los valores t+1, t+2, t+3, t+4, t+5, t+6\n",
    "enso_class_grouped = enso_class_grouped.shift(time=-1)\n",
    "enso_class_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de haber realizado todas las operaciones de agrupación, debemos de verificar que los datos esten alineados en el tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_time = max(enso_class_grouped.time.min(), sst_grouped.time.min())\n",
    "max_time = min(enso_class_grouped.time.max(), sst_grouped.time.max())\n",
    "\n",
    "sst_grouped_orig = sst_grouped.copy()\n",
    "sst_grouped = sst_grouped.sel(time=slice(min_time, max_time))\n",
    "en34_grouped = enso_class_grouped.sel(time=slice(min_time, max_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos de entrada no deben de tener valores faltantes, por lo que llenaremos los valores faltantes con 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_grouped = sst_grouped.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploramos una muestra que sera usada en el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_grouped[0].plot(col=\"channel\", col_wrap=4, cmap=\"coolwarm\", vmin=-3, vmax=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enso_class_grouped[0].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, cabe recordar que necesitamos tener estos datos en un formato que el modelo pueda entender, por lo que necesitamos usar one-hot encoding para las etiquetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_enso = tf.keras.utils.to_categorical(en34_grouped, 3)\n",
    "one_hot_enso.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separación de datos\n",
    "\n",
    "Una vez que hemos agrupado los datos en secuencias de 12 meses para las SSTA y 6 meses para el índice EN3.4, debemos separar los datos en conjuntos de entrenamiento y prueba. Haremos una division de 80/20, donde el 80% de los datos se utilizarán para entrenar el modelo y el 20% restante se utilizará para evaluar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(sst_grouped))\n",
    "\n",
    "sst_train = sst_grouped.isel(time=slice(None, train_size)).values\n",
    "enso_train = one_hot_enso[:train_size]\n",
    "\n",
    "sst_test = sst_grouped.isel(time=slice(train_size, None)).values\n",
    "enso_test = one_hot_enso[train_size:]\n",
    "\n",
    "print(f\"Train shape: {sst_train.shape}, {enso_train.shape}\")\n",
    "print(f\"Test shape: {sst_test.shape}, {enso_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos todos los datos preparados, podemos comenzar a construir el modelo de red neuronal convolucional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear el modelo\n",
    "\n",
    "Similar al ejemplo anterior con el conjunto de datos MNIST, definimos la arquitectura del modelo con múltiples capas convolucionales seguidas por capas de agrupación y finalmente una capa completamente conectada. En este caso en particular, el modelo tendra 6 salidas, una para cada mes pronosticado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_shape):\n",
    "    model_input = tf.keras.layers.Input(shape=input_shape)\n",
    "    x = tf.keras.layers.Conv2D(16, (3, 3), activation=\"relu\")(model_input)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    x = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "\n",
    "    x_lead1 = tf.keras.layers.Dense(3, name=\"lead1\")(x)\n",
    "    x_lead2 = tf.keras.layers.Dense(3, name=\"lead2\")(x)\n",
    "    x_lead3 = tf.keras.layers.Dense(3, name=\"lead3\")(x)\n",
    "    x_lead4 = tf.keras.layers.Dense(3, name=\"lead4\")(x)\n",
    "    x_lead5 = tf.keras.layers.Dense(3, name=\"lead5\")(x)\n",
    "    x_lead6 = tf.keras.layers.Dense(3, name=\"lead6\")(x)\n",
    "\n",
    "    model = tf.keras.Model(\n",
    "        inputs=[model_input],\n",
    "        outputs=[x_lead1, x_lead2, x_lead3, x_lead4, x_lead5, x_lead6],\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compilar y entrenar el modelo\n",
    "\n",
    "Una vez que hemos definido la arquitectura del modelo, lo compilamos y lo entrenamos con los datos de entrenamiento. En este caso, utilizamos el optimizador Adam y la función de pérdida de entropía cruzada categórica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enso_model = get_model(sst_train.shape[1:])\n",
    "enso_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enso_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"] * 6,\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 15\n",
    "\n",
    "history = enso_model.fit(\n",
    "    sst_train,\n",
    "    [enso_train[:, i] for i in range(6)],\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.1,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluar el modelo\n",
    "\n",
    "Una vez que hemos entrenado el modelo, evaluamos su rendimiento en el conjunto de datos de prueba. En este caso, utilizamos la precisión como métrica de evaluación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "ax[0].plot(history.history[\"lead1_accuracy\"], label=\"train\")\n",
    "ax[0].plot(history.history[\"val_lead1_accuracy\"], label=\"validation\")\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Accuracy\")\n",
    "\n",
    "ax[1].plot(history.history[\"loss\"], label=\"train\")\n",
    "ax[1].plot(history.history[\"val_loss\"], label=\"validation\")\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].set_ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay muchos factores que pueden afectar el rendimiento de la CNN. En este caso puede que los datos que le estemos otorgando a la red tengan demasiado detalle para el tipo de problema que estamos tratando de resolver. En este caso, la red puede estar aprendiendo demasiado sobre los datos de entrenamiento y no generalizando lo suficiente para los datos de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = enso_model.evaluate(sst_test, [enso_test[:, i] for i in range(6)])\n",
    "print(\"Test loss:\", score[0])\n",
    "\n",
    "print(f\"Lead 1 accuracy: {score[1]*100:.2f}%\")\n",
    "print(f\"Lead 2 accuracy: {score[2]*100:.2f}%\")\n",
    "print(f\"Lead 3 accuracy: {score[3]*100:.2f}%\")\n",
    "print(f\"Lead 4 accuracy: {score[4]*100:.2f}%\")\n",
    "print(f\"Lead 5 accuracy: {score[5]*100:.2f}%\")\n",
    "print(f\"Lead 6 accuracy: {score[6]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediccion con los valores más recientes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable `sst_grouped_orig` contiene los datos mas recientes de SST, por lo que podemos usar estos datos para hacer predicciones sobre la fase de ENOS en los próximos 6 meses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_data = sst_grouped_orig.isel(time=-1).fillna(0)\n",
    "latest_data = latest_data.values[None, ...]\n",
    "latest_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implemente la prediccion de los 6 meses siguientes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meteo_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
