{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularización en Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/griverat/Meteo-AI/blob/main/notebooks/2.regularization.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Si usa Google Colab, asegúrese de tener habilitada la GPU para este notebook.**\n",
    "\n",
    "![gpu_colab](../images/colab_gpu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción\n",
    "\n",
    "En este notebook vamos a ver cómo la regularización puede ayudar a mejorar el rendimiento de un modelo de Machine Learning. Para ello, vamos a utilizar un dataset de ejemplo y vamos a entrenar un modelo sin regularización y otro con regularización, explorando cómo se comportan ambos modelos.\n",
    "\n",
    "## Objetivos\n",
    "\n",
    "- Entender qué es la regularización y cómo se aplica en Machine Learning.\n",
    "- Comparar distintos mecanismos de regularización.\n",
    "- Entrenar un modelo con y sin regularización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Qué es la regularización?\n",
    "\n",
    "La regularización es una técnica utilizada en Machine Learning para evitar el sobreajuste de los modelos. El sobreajuste ocurre cuando un modelo se ajusta demasiado bien a los datos de entrenamiento, lo que puede llevar a que el modelo tenga un rendimiento deficiente en datos nuevos o no vistos.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Overfitting.svg/1280px-Overfitting.svg.png\" width=\"400\"></img>\n",
    "\n",
    "La línea verde representa un modelo sobreajustado y la línea negra representa un modelo regularizado. Si bien la línea verde sigue mejor los datos de entrenamiento, depende demasiado de esos datos y es probable que tenga una tasa de error más alta en datos nuevos no vistos, ilustrados por puntos delineados en negro, en comparación con la línea negra.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen diferentes métodos de regularización que se pueden aplicar a los modelos de aprendizaje automático. Dos de los métodos más comunes son la regularización L1 y la regularización L2.\n",
    "\n",
    "La regularización L1, también conocida como regularización de Lasso, agrega una penalización a la función de costo del modelo basada en la suma de los valores absolutos de los coeficientes del modelo. Esto tiene el efecto de forzar algunos coeficientes a cero, lo que puede conducir a la selección automática de características y simplificación del modelo.\n",
    "\n",
    "La regularización L2, también conocida como regularización de Ridge, agrega una penalización a la función de costo del modelo basada en la suma de los cuadrados de los coeficientes del modelo. Esto tiene el efecto de reducir los valores de los coeficientes, lo que puede ayudar a evitar el sobreajuste y mejorar la generalización del modelo.\n",
    "\n",
    "Además de la regularización L1 y L2, también existen otros métodos de regularización, como la elastic net, que combina las penalizaciones de L1 y L2, y la regularización de dropout, que apaga aleatoriamente algunas neuronas durante el entrenamiento para evitar la dependencia excesiva de ciertas características.\n",
    "\n",
    "La regularización es especialmente útil cuando se trabaja con conjuntos de datos pequeños o cuando hay una alta dimensionalidad de características. Al agregar una penalización a la función de costo, la regularización ayuda a controlar la complejidad del modelo y a evitar el sobreajuste, lo que puede resultar en modelos más estables y con mejor rendimiento en datos nuevos.\n",
    "\n",
    "En resumen, la regularización es una técnica importante en el aprendizaje automático que ayuda a evitar el sobreajuste y mejorar la generalización del modelo. Al aplicar una penalización a la función de costo, la regularización controla la complejidad del modelo y promueve la selección de características relevantes, lo que puede conducir a modelos más estables y con mejor rendimiento en datos nuevos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularización en Tensorflow\n",
    "\n",
    "La regularización implementada en tensorflow, mediante la interfaz de Keras, es aplicada independientemente por capas. Esto indica que dependiendo de como nos gustase configurar el modelo, podemos escoger distintas formas de regularización con distintas intensidades para cada capa de nuestro modelo.\n",
    "\n",
    "En Keras, existen tres tipos de regularización que se pueden aplicar a las capas de un modelo: regularización del kernel, regularización del bias y regularización de la actividad.\n",
    "\n",
    "1. Regularización del kernel:\n",
    "La regularización del kernel se aplica a los pesos de las conexiones entre las neuronas de una capa. Ayuda a controlar la complejidad del modelo penalizando los valores grandes de los pesos. Esto se logra mediante la adición de una penalización a la función de pérdida del modelo. La regularización del kernel se puede utilizar para evitar el sobreajuste y mejorar la generalización del modelo.\n",
    "\n",
    "2. Regularización del bias:\n",
    "La regularización del bias se aplica a los términos de sesgo de las neuronas de una capa. Al igual que la regularización del kernel, la regularización del bias ayuda a controlar la complejidad del modelo y evitar el sobreajuste. Se puede utilizar para reducir la dependencia excesiva de ciertas características y mejorar la capacidad de generalización del modelo.\n",
    "\n",
    "3. Regularización de la actividad:\n",
    "La regularización de la actividad se aplica a la salida de una capa. Ayuda a controlar la magnitud de las activaciones de las neuronas, evitando así valores extremadamente grandes o pequeños. Esto puede ser útil para evitar la saturación de las funciones de activación y mejorar la estabilidad del modelo.\n",
    "\n",
    "Para aplicar la regularización en Keras, se pueden utilizar diferentes técnicas, como la regularización L1, la regularización L2 o la regularización de dropout. Estas técnicas se pueden especificar al definir una capa en Keras, utilizando los parámetros correspondientes, como `kernel_regularizer`, `bias_regularizer` y `activity_regularizer`.\n",
    "\n",
    "```python\n",
    "from tensorflow.keras import layers, regularizers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01),\n",
    "                 bias_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "```\n",
    "\n",
    "En este ejemplo, se utiliza la regularización L2 con un factor de penalización de 0.01 para la regularización del kernel, el bias y la actividad de la capa densa. Esto ayudará a controlar la complejidad del modelo y evitar el sobreajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación\n",
    "\n",
    "Para demostrar el efecto de la regularización en un modelo de Machine Learning, vamos a utilizar un dataset sintético creado usando `sklearn` para entrenar un modelo clasificación binaria. Al usar un set de datos sintético, podemos controlar el nivel de ruido y la complejidad del problema, lo que nos permitirá ver cómo la regularización actúa realmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import make_circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = make_circles(n_samples=100, noise=0.1, random_state=1)\n",
    "\n",
    "df = pd.DataFrame(dict(x=x[:, 0], y=x[:, 1], label=y))\n",
    "colors = {0: \"red\", 1: \"blue\"}\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "grouped = df.groupby(\"label\")\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax, kind=\"scatter\", x=\"x\", y=\"y\", label=key, color=colors[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como siempre, antes de comenzar a definir los modelos a usar, debemos separar nuestros datos en al menos dos conjuntos: uno de entrenamiento y otro de prueba. Para este ejemplo vamos a exagerar y usar 30 ejemplos para entrenar y 70 para probar.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = x[:30], x[30:]\n",
    "y_train, y_test = y[:30], y[30:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sobreajuste de perceptrón multicapa\n",
    "\n",
    "Vamos a desarrollar un perceptrón multicapa para resolver este problema de clasificación binaria. El modelo tendrá una sola capa con más neuronas de las necesarias para resolver el problema, lo que lo hará propenso a sobreajustarse a los datos de entrenamiento. Adicionalmente, entrenaremos el modelo durante un número de épocas mayor al necesario para que el sobreajuste sea más evidente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.InputLayer(input_shape=(2,)))\n",
    "model.add(tf.keras.layers.Dense(500, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo definido va a set entrenado por 4000 epocas con un batch size de 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x_train, y_train, validation_data=(x_test, y_test), epochs=4000, verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos revisar las métricas de entrenamiento, en donde se nota claramente como el modelo se ajusta a los datos de entrenamiento, pero no logra generalizar bien a los datos de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "ax[0].plot(history.history[\"loss\"], label=\"train\")\n",
    "ax[0].plot(history.history[\"val_loss\"], label=\"test\")\n",
    "ax[0].legend()\n",
    "ax[0].set_title(\"loss\")\n",
    "\n",
    "ax[1].plot(history.history[\"accuracy\"], label=\"train\")\n",
    "ax[1].plot(history.history[\"val_accuracy\"], label=\"test\")\n",
    "ax[1].legend()\n",
    "ax[1].set_title(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos visualizar la frontera de decisión del modelo, la cual se ajusta muy bien a los datos de entrenamiento, pero no logra generalizar bien a los datos de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouded_train = pd.DataFrame(dict(x=x_train[:, 0], y=x_train[:, 1], label=y_train))\n",
    "grouped_test = pd.DataFrame(dict(x=x_test[:, 0], y=x_test[:, 1], label=y_test))\n",
    "grouped_train = grouded_train.groupby(\"label\")\n",
    "grouped_test = grouped_test.groupby(\"label\")\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "for key, group in grouped_train:\n",
    "    group.plot(ax=axs[0], kind=\"scatter\", x=\"x\", y=\"y\", label=key, color=colors[key])\n",
    "for key, group in grouped_test:\n",
    "    group.plot(ax=axs[1], kind=\"scatter\", x=\"x\", y=\"y\", label=key, color=colors[key])\n",
    "\n",
    "for dset, ax in zip([x_train, x_test], axs):\n",
    "    x_min, x_max = dset[:, 0].min() - 1, dset[:, 0].max() + 1\n",
    "    y_min, y_max = dset[:, 1].min() - 1, dset[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])[:, 0]\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contour(xx, yy, Z, levels=[0.5], linestyles=\"dashed\")\n",
    "\n",
    "axs[0].set_title(\"train\")\n",
    "axs[1].set_title(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
    "_, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Train: {train_acc:.1%}, Test: {test_acc:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularización en el modelo sobreajustado\n",
    "\n",
    "Ahora vamos a ver como los distinto algoritmos de regularización pueden ayudar a mejorar el rendimiento del modelo. Para ello, vamos a entrenar el mismo modelo, pero con regularización L1 y L2 en las capas densas.\n",
    "\n",
    "Para comenzar, debemos usar una activación lineal en la capa densa de la red, ya que queremos aplicar la regularización sobre los datos en crudo, antes de pasar por la función de activación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.InputLayer(input_shape=(2,)))\n",
    "model.add(\n",
    "    tf.keras.layers.Dense(\n",
    "        500, activation=\"linear\", activity_regularizer=tf.keras.regularizers.l1(0.0001)\n",
    "    )\n",
    ")\n",
    "model.add(tf.keras.layers.Activation(\"relu\"))\n",
    "model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora realizamos el entrenamiento para evaluar el cambio en el rendimiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x_train, y_train, validation_data=(x_test, y_test), epochs=4000, verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "ax[0].plot(history.history[\"loss\"], label=\"train\")\n",
    "ax[0].plot(history.history[\"val_loss\"], label=\"test\")\n",
    "ax[0].legend()\n",
    "ax[0].set_title(\"loss\")\n",
    "\n",
    "ax[1].plot(history.history[\"accuracy\"], label=\"train\")\n",
    "ax[1].plot(history.history[\"val_accuracy\"], label=\"test\")\n",
    "ax[1].legend()\n",
    "ax[1].set_title(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
    "_, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Train: {train_acc:.1%}, Test: {test_acc:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouded_train = pd.DataFrame(dict(x=x_train[:, 0], y=x_train[:, 1], label=y_train))\n",
    "grouped_test = pd.DataFrame(dict(x=x_test[:, 0], y=x_test[:, 1], label=y_test))\n",
    "grouped_train = grouded_train.groupby(\"label\")\n",
    "grouped_test = grouped_test.groupby(\"label\")\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "for key, group in grouped_train:\n",
    "    group.plot(ax=axs[0], kind=\"scatter\", x=\"x\", y=\"y\", label=key, color=colors[key])\n",
    "for key, group in grouped_test:\n",
    "    group.plot(ax=axs[1], kind=\"scatter\", x=\"x\", y=\"y\", label=key, color=colors[key])\n",
    "\n",
    "for dset, ax in zip([x_train, x_test], axs):\n",
    "    x_min, x_max = dset[:, 0].min() - 1, dset[:, 0].max() + 1\n",
    "    y_min, y_max = dset[:, 1].min() - 1, dset[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])[:, 0]\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contour(xx, yy, Z, levels=[0.5], linestyles=\"dashed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De manera similar, podemos hacer uso de la regularización L2 en la capa densa para ver cómo afecta al rendimiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.InputLayer(input_shape=(2,)))\n",
    "model.add(\n",
    "    tf.keras.layers.Dense(\n",
    "        500, activation=\"linear\", activity_regularizer=tf.keras.regularizers.l2(0.0001)\n",
    "    )\n",
    ")\n",
    "model.add(tf.keras.layers.Activation(\"relu\"))\n",
    "model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora realizamos el entrenamiento para evaluar el cambio en el rendimiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x_train, y_train, validation_data=(x_test, y_test), epochs=4000, verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "ax[0].plot(history.history[\"loss\"], label=\"train\")\n",
    "ax[0].plot(history.history[\"val_loss\"], label=\"test\")\n",
    "ax[0].legend()\n",
    "ax[0].set_title(\"loss\")\n",
    "\n",
    "ax[1].plot(history.history[\"accuracy\"], label=\"train\")\n",
    "ax[1].plot(history.history[\"val_accuracy\"], label=\"test\")\n",
    "ax[1].legend()\n",
    "ax[1].set_title(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
    "_, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Train: {train_acc:.1%}, Test: {test_acc:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouded_train = pd.DataFrame(dict(x=x_train[:, 0], y=x_train[:, 1], label=y_train))\n",
    "grouped_test = pd.DataFrame(dict(x=x_test[:, 0], y=x_test[:, 1], label=y_test))\n",
    "grouped_train = grouded_train.groupby(\"label\")\n",
    "grouped_test = grouped_test.groupby(\"label\")\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "for key, group in grouped_train:\n",
    "    group.plot(ax=axs[0], kind=\"scatter\", x=\"x\", y=\"y\", label=key, color=colors[key])\n",
    "for key, group in grouped_test:\n",
    "    group.plot(ax=axs[1], kind=\"scatter\", x=\"x\", y=\"y\", label=key, color=colors[key])\n",
    "\n",
    "for dset, ax in zip([x_train, x_test], axs):\n",
    "    x_min, x_max = dset[:, 0].min() - 1, dset[:, 0].max() + 1\n",
    "    y_min, y_max = dset[:, 1].min() - 1, dset[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])[:, 0]\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contour(xx, yy, Z, levels=[0.5], linestyles=\"dashed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos verificar que pese a usar regularizacion L2 para este problema, el modelo sigue sobreajustando los datos de entrenamiento, indicando que es mejor cortar las conexiones con la regularización L1 que reducir su peso con la regularización L2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio\n",
    "\n",
    "Hemos realizado pruebas con el regularizador L1 y L2, ahora te toca a ti implementar la regularización ElasticNet (combinación de L1 y L2) para ver cómo afecta al rendimiento del modelo.\n",
    "\n",
    "Keras proprociona la función `regularizers.L1L2`, la cual toma como parámetros `l1` y `l2`, los cuales son los factores de penalización para la regularización L1 y L2, respectivamente.\n",
    "\n",
    "```python\n",
    "tf.keras.regularizers.L1L2(l1=0.0, l2=0.0)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "Vamos a retomar uno de los ejemplos iniciales que realizamos en la introducción a Keras, en donde entrenamos un modelo de perceptrón multicapa para el pronóstico de la temperatura en la siguiente hora. En este caso, vamos a aplicar la técnica de dropout para evitar el sobreajuste del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data = pd.read_csv(\"data/campo_de_marte.csv\", skiprows=10)\n",
    "\n",
    "# renombramos las columnas a algo más amigable\n",
    "station_data.columns = [\n",
    "    \"date\",\n",
    "    \"hour\",\n",
    "    \"temp\",\n",
    "    \"precip\",\n",
    "    \"humidity\",\n",
    "    \"wind_dir\",\n",
    "    \"wind_speed\",\n",
    "]\n",
    "\n",
    "# combinamos las columnas de fecha y hora en una sola\n",
    "station_data[\"date\"] = pd.to_datetime(station_data[\"date\"] + \" \" + station_data[\"hour\"])\n",
    "station_data = station_data.drop(columns=[\"hour\"])\n",
    "\n",
    "# convertimos las columnas de temperatura, precipitación, humedad y velocidad del viento a números\n",
    "station_data[\"temp\"] = pd.to_numeric(\n",
    "    station_data[\"temp\"], errors=\"coerce\", downcast=\"float\"\n",
    ")\n",
    "station_data[\"precip\"] = pd.to_numeric(\n",
    "    station_data[\"precip\"], errors=\"coerce\", downcast=\"float\"\n",
    ")\n",
    "station_data[\"humidity\"] = pd.to_numeric(\n",
    "    station_data[\"humidity\"], errors=\"coerce\", downcast=\"float\"\n",
    ")\n",
    "station_data[\"wind_dir\"] = pd.to_numeric(\n",
    "    station_data[\"wind_dir\"], errors=\"coerce\", downcast=\"float\"\n",
    ")\n",
    "station_data[\"wind_speed\"] = pd.to_numeric(\n",
    "    station_data[\"wind_speed\"], errors=\"coerce\", downcast=\"float\"\n",
    ")\n",
    "\n",
    "# eliminamos las filas con valores faltantes\n",
    "station_data = station_data.dropna()\n",
    "station_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data[\"next_temp\"] = station_data[\"temp\"].shift(-1)\n",
    "station_data = station_data.dropna()\n",
    "station_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta este punto hemos copiado el código que usamos en la primera sesión para cargar los datos y preprocesarlos.\n",
    "En esta oportunidad, vamos a agregar dos datos adicionales de entrada que sera el Mes, el Dia y la Hora de la medición en forma de armónicos. Esto ayudara al modelo a capturar la estacionalidad de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data[\"hour_sin\"] = np.sin(2 * np.pi / (station_data.date.dt.hour + 1e-3))\n",
    "station_data[\"hour_cos\"] = np.cos(2 * np.pi / (station_data.date.dt.hour + 1e-3))\n",
    "station_data[\"day_sin\"] = np.sin(2 * np.pi / station_data.date.dt.day_of_year)\n",
    "station_data[\"day_cos\"] = np.cos(2 * np.pi / station_data.date.dt.day_of_year)\n",
    "station_data[\"month_sin\"] = np.sin(2 * np.pi / station_data.date.dt.month)\n",
    "station_data[\"month_cos\"] = np.cos(2 * np.pi / station_data.date.dt.month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(station_data) * 0.7)\n",
    "val_size = int(len(station_data) * 0.15)\n",
    "test_size = len(station_data) - train_size - val_size\n",
    "\n",
    "input_vars = [\n",
    "    \"temp\",\n",
    "    \"humidity\",\n",
    "    \"wind_speed\",\n",
    "    \"hour_sin\",\n",
    "    \"hour_cos\",\n",
    "    \"day_sin\",\n",
    "    \"day_cos\",\n",
    "    \"month_sin\",\n",
    "    \"month_cos\",\n",
    "]\n",
    "\n",
    "train_data = station_data[input_vars].iloc[:train_size]\n",
    "train_label = station_data[\"next_temp\"].iloc[:train_size]\n",
    "\n",
    "val_data = station_data[input_vars].iloc[train_size : train_size + val_size]\n",
    "val_label = station_data[\"next_temp\"].iloc[train_size : train_size + val_size]\n",
    "\n",
    "test_data = station_data[input_vars].iloc[train_size + val_size :]\n",
    "test_label = station_data[\"next_temp\"].iloc[train_size + val_size :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = train_data.mean()\n",
    "std = train_data.std()\n",
    "\n",
    "train_data_standarized = (train_data - mean) / std\n",
    "val_data_standarized = (val_data - mean) / std\n",
    "test_data_standarized = (test_data - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora creamos nuestra red neuronal sin dropout como linea base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_baseline = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1),\n",
    "    ]\n",
    ")\n",
    "model_baseline.build(input_shape=(None, len(input_vars)))\n",
    "\n",
    "model_baseline.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "history = model_baseline.fit(\n",
    "    train_data_standarized,\n",
    "    train_label,\n",
    "    epochs=100,\n",
    "    validation_data=(val_data_standarized, val_label),\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluamos el rendimiento del modelo y visualizamos las métricas de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "ax[0].plot(history.history[\"loss\"], label=\"train\")\n",
    "ax[0].plot(history.history[\"val_loss\"], label=\"test\")\n",
    "ax[0].legend()\n",
    "ax[0].set_title(\"loss\")\n",
    "\n",
    "ax[1].plot(history.history[\"mae\"], label=\"train\")\n",
    "ax[1].plot(history.history[\"val_mae\"], label=\"test\")\n",
    "ax[1].legend()\n",
    "ax[1].set_title(\"mae\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora revisamos que tan preciso es el modelo en la predicción de la temperatura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, baseline_mae = model_baseline.evaluate(test_data_standarized, test_label, verbose=0)\n",
    "print(f\"Baseline MAE: {baseline_mae:.2f}\")\n",
    "preds_baseline = model_baseline.predict(test_data_standarized)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(test_label.values, label=\"true\")\n",
    "ax.plot(preds_baseline, label=\"baseline\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora definiremos nuestra red neuronal con dropout en la capa oculta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dropout = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(1),\n",
    "    ]\n",
    ")\n",
    "model_dropout.build(input_shape=(None, len(input_vars)))\n",
    "\n",
    "model_dropout.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "history = model_dropout.fit(\n",
    "    train_data_standarized,\n",
    "    train_label,\n",
    "    epochs=100,\n",
    "    validation_data=(val_data_standarized, val_label),\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "ax[0].plot(history.history[\"loss\"], label=\"train\")\n",
    "ax[0].plot(history.history[\"val_loss\"], label=\"test\")\n",
    "ax[0].legend()\n",
    "ax[0].set_title(\"loss\")\n",
    "\n",
    "ax[1].plot(history.history[\"mae\"], label=\"train\")\n",
    "ax[1].plot(history.history[\"val_mae\"], label=\"test\")\n",
    "ax[1].legend()\n",
    "ax[1].set_title(\"mae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, dropout_mae = model_dropout.evaluate(test_data_standarized, test_label, verbose=0)\n",
    "print(f\"Dropout MAE: {dropout_mae:.2f}\")\n",
    "preds_dropout = model_dropout.predict(test_data_standarized)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(test_label.values, label=\"true\")\n",
    "ax.plot(preds_dropout, label=\"baseline\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar como nuestro modelo con dropout logra generalizar mejor a los datos de prueba, lo que se refleja en una menor pérdida y un mayor rendimiento en la métrica de precisión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping\n",
    "\n",
    "El entrenamiento de un modelo de Machine Learning puede ser un proceso costoso en términos de tiempo y recursos computacionales. Por esta razón, es importante tener en cuenta estrategias para detener el entrenamiento de un modelo cuando ya no se observa una mejora significativa en su rendimiento.\n",
    "\n",
    "Una de las estrategias más comunes para detener el entrenamiento de un modelo es el uso de la técnica de early stopping. Esta técnica consiste en monitorear una métrica de rendimiento, como la pérdida en el conjunto de validación, y detener el entrenamiento del modelo cuando la métrica deja de mejorar.\n",
    "\n",
    "En Keras, la técnica de early stopping se puede implementar utilizando el callback `EarlyStopping`. Este callback permite especificar la métrica a monitorear, el número de épocas para esperar antes de detener el entrenamiento y otras opciones como la paciencia y el modo de monitoreo.\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode='min')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este ejemplo vamos a usar una configuracion la misma cantidad de neuronas usadas en los ejemplos anteriores, pero con una cantidad de epocas de entrenamiento de 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1),\n",
    "    ]\n",
    ")\n",
    "model.build(input_shape=(None, len(input_vars)))\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos el callback de early stopping y lo agregamos a la lista de callbacks al entrenar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=30, restore_best_weights=True\n",
    "    )\n",
    "]\n",
    "history = model.fit(\n",
    "    train_data_standarized,\n",
    "    train_label,\n",
    "    epochs=1000,\n",
    "    validation_data=(val_data_standarized, val_label),\n",
    "    verbose=1,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar como el modelo se detiene antes de las 100 épocas, ya que no se observa una mejora significativa en la pérdida en el conjunto de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "ax[0].plot(history.history[\"loss\"], label=\"train\")\n",
    "ax[0].plot(history.history[\"val_loss\"], label=\"test\")\n",
    "ax[0].axvline(\n",
    "    x=len(history.history[\"loss\"]) - 10,\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    label=\"early stopping\",\n",
    ")\n",
    "ax[0].legend()\n",
    "ax[0].set_title(\"loss\")\n",
    "\n",
    "ax[1].plot(history.history[\"mae\"], label=\"train\")\n",
    "ax[1].plot(history.history[\"val_mae\"], label=\"test\")\n",
    "ax[1].legend()\n",
    "ax[1].set_title(\"mae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, mae = model.evaluate(test_data_standarized, test_label, verbose=0)\n",
    "print(f\"Test MAE: {mae:.2f}\")\n",
    "preds = model.predict(test_data_standarized)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(test_label.values, label=\"true\")\n",
    "ax.plot(preds, label=\"Prediction\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general, es recomendable el uso de Early Stopping en todo momento para evitar el sobreajuste de los modelos y reducir el tiempo de entrenamiento. Sin embargo, es importante tener en cuenta que el uso de Early Stopping puede llevar a detener el entrenamiento antes de que el modelo haya convergido completamente, por lo que es importante ajustar los parámetros de paciencia y modo de monitoreo para obtener los mejores resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio\n",
    "\n",
    "Usando la misma configuracion del modelo anterior, modifique los parametros de early stopping para que:\n",
    "\n",
    "- Monitoree la métrica de mae en el conjunto de validación.\n",
    "- Comience a monitorear después de 50 épocas.\n",
    "\n",
    "Para ello, puede consultar la [documentación](https://keras.io/api/callbacks/early_stopping/) de Keras sobre el callback `EarlyStopping` para ver cómo configurar los parámetros adecuadamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization\n",
    "\n",
    "Batch normalization es una técnica utilizada en redes neuronales para normalizar las activaciones de una capa oculta en mini-batches de datos. Esto ayuda a estabilizar y acelerar el entrenamiento de la red, ya que evita que las activaciones se vuelvan demasiado grandes o pequeñas, lo que puede llevar a problemas de convergencia y desvanecimiento del gradiente.\n",
    "\n",
    "En Keras, la capa de batch normalization se puede agregar a un modelo utilizando la clase `BatchNormalization`. Esta capa se puede agregar después de una capa de activación en una red neuronal y normaliza las activaciones de la capa anterior en mini-batches de datos.\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "```\n",
    "\n",
    "Nuevamente, usando la configuracion de la red neuronal anterior, vamos a agregar una capa de batch normalization después de la capa de activación en la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(1),\n",
    "    ]\n",
    ")\n",
    "model.build(input_shape=(None, len(input_vars)))\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=30, restore_best_weights=True\n",
    "    )\n",
    "]\n",
    "history = model.fit(\n",
    "    train_data_standarized,\n",
    "    train_label,\n",
    "    epochs=1000,\n",
    "    validation_data=(val_data_standarized, val_label),\n",
    "    verbose=1,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "ax[0].plot(history.history[\"loss\"], label=\"train\")\n",
    "ax[0].plot(history.history[\"val_loss\"], label=\"test\")\n",
    "ax[0].axvline(\n",
    "    x=len(history.history[\"loss\"]) - 10,\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    label=\"early stopping\",\n",
    ")\n",
    "ax[0].legend()\n",
    "ax[0].set_title(\"loss\")\n",
    "\n",
    "ax[1].plot(history.history[\"mae\"], label=\"train\")\n",
    "ax[1].plot(history.history[\"val_mae\"], label=\"test\")\n",
    "ax[1].legend()\n",
    "ax[1].set_title(\"mae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, mae = model.evaluate(test_data_standarized, test_label, verbose=0)\n",
    "print(f\"Test MAE: {mae:.2f}\")\n",
    "preds = model.predict(test_data_standarized)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(test_label.values, label=\"true\")\n",
    "ax.plot(preds, label=\"Prediction\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Cómo afecta la capa de batch normalization al rendimiento del modelo?**\n",
    "\n",
    "Podemos observar cómo la capa de batch normalization ayuda a estabilizar y acelerar el entrenamiento de la red, lo que se refleja en una menor pérdida y un mayor rendimiento en la métrica de precisión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio\n",
    "\n",
    "Modifique la red neuronal anterior para usar la capa de batch normalization solamente después de la capa de entrada. ¿Cómo afecta esto al rendimiento del modelo?\n",
    "\n",
    "Ahora intente combinar la capa de batch normalization con la técnica de dropout en la red neuronal. ¿Cómo afecta esto al rendimiento del modelo? ¿El orden en que se aplican las técnicas afecta al rendimiento del modelo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meteo_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
