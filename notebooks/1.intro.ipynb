{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inteligencia Artificial aplicada a la Meteorología"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/griverat/Meteo-AI/blob/main/notebooks/1.intro.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción\n",
    "\n",
    "Este notebook contiene el material a desarrollar durante la primera sesión del taller de Inteligencia Artificial aplicada a la Meteorología. Se presentarán los conceptos básicos de la inteligencia artificial y se describirán los pasos a seguir para implementar modelos simples en Pytorch y Tensorflow.\n",
    "\n",
    "Cabe notar que no se hara uso del paquete `sklearn` ya que la finalidad de este taller es el poder entender e implementar modelos de Deep Learning los cuales son más complejos y poderosos que los modelos tradicionales (como los de `sklearn`). Sin embargo, esto no implica que los modelos simples no sean útiles, de hecho, en muchos casos los modelos simples son suficientes para resolver problemas de clasificación y regresión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos\n",
    "\n",
    "- Entender los conceptos básicos de la inteligencia artificial.\n",
    "- Implementar un modelo simple en Pytorch.\n",
    "- Implementar un modelo simple en Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solo correr esta celda si se usa google colab\n",
    "# Quitar el comentario (#) a los comandos que comienzan con !\n",
    "\n",
    "# !mkdir data\n",
    "# !wget https://raw.githubusercontent.com/griverat/Meteo-AI/main/data/campo_de_marte.csv -O data/campo_de_marte.csv\n",
    "\n",
    "# !pip install ydata_profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción a la Inteligencia Artificial\n",
    "\n",
    "La inteligencia artificial (IA) es una rama de la informática que se encarga de desarrollar algoritmos y modelos que permiten a las máquinas aprender de los datos y realizar tareas que normalmente requieren de la inteligencia humana. Existen diferentes tipos de algoritmos de IA, sin embargo, en este taller nos enfocaremos en los modelos de Deep Learning.\n",
    "\n",
    "Los modelos de Deep Learning son un subconjunto de los modelos de IA que se basan en redes neuronales artificiales. Estos modelos son capaces de aprender patrones complejos en los datos y realizar tareas como clasificación, regresión, segmentación, entre otros.\n",
    "\n",
    "Para poder implementar un modelo de Deep Learning es necesario seguir los siguientes pasos:\n",
    "\n",
    "1. Preprocesar los datos.\n",
    "2. Definir el modelo.\n",
    "3. Entrenar el modelo.\n",
    "4. Evaluar el modelo.\n",
    "5. Hacer predicciones.\n",
    "\n",
    "En este notebook se presentarán los pasos 2 y 3 para implementar un modelo simple (ANN) en Pytorch y Tensorflow. Para ello, se utilizará un conjunto de datos de ejemplo que contiene datos de la estacion meteorológica ubicada en Campo de Marte (Lima, Perú) para el año 2024."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch vs Tensorflow: ¿Cuál elegir?\n",
    "\n",
    "Pytorch y Tensorflow son dos de los frameworks más populares para implementar modelos de Deep Learning. Ambos frameworks tienen ventajas y desventajas, por lo que la elección de uno u otro dependerá de las necesidades y preferencias del desarrollador.\n",
    "\n",
    "Pytorch es un framework de código abierto desarrollado por Facebook que se caracteriza por su facilidad de uso y flexibilidad. Pytorch utiliza un enfoque dinámico para definir y entrenar modelos, lo que permite a los desarrolladores modificar el grafo computacional en tiempo de ejecución. Esto hace que Pytorch sea ideal para prototipar modelos y experimentar con diferentes arquitecturas.\n",
    "\n",
    "Tensorflow, por otro lado, es un framework de código abierto desarrollado por Google que se caracteriza por su escalabilidad y eficiencia. Tensorflow utiliza un enfoque estático para definir y entrenar modelos, lo que permite a los desarrolladores optimizar el grafo computacional antes de ejecutarlo. Esto hace que Tensorflow sea ideal para entrenar modelos en grandes conjuntos de datos y desplegarlos en entornos de producción.\n",
    "\n",
    "En pocas palabras, la elección que hagamos dependerá de nuestras necesidades y preferencias. El notebook de hoy nos permitirá ver cómo implementar un modelo simple en Pytorch y Tensorflow, lo que nos dará una idea de cómo se trabaja con cada uno de estos frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.polynomial.polynomial as poly\n",
    "import pandas as pd\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"monospace\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura de datos\n",
    "\n",
    "La data que utilizaremos en este notebook fue descargada del Servicio Nacional de Meteorología e Hidrología del Perú (SENAMHI) y contiene información de la estación meteorológica ubicada en Campo de Marte (Lima, Perú) para el año 2024. La data contiene las siguientes variables:\n",
    "\n",
    "- Fecha: Fecha en formato `YYYY/MM/DD`.\n",
    "- Hora: Hora en formato `HH:MM`.\n",
    "- Temperatura: Temperatura en grados Celsius.\n",
    "- Precipitación: Precipitación en milímetros por hora.\n",
    "- Humedad: Humedad relativa en porcentaje.\n",
    "- Dirección del viento: Dirección del viento en grados.\n",
    "- Velocidad del viento: Velocidad del viento en metros por segundo.\n",
    "\n",
    "Para leer la data utilizaremos la librería `pandas` y mostraremos las primeras 5 filas del dataset para tener una idea de cómo se ve la data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data = pd.read_csv(\"campo_de_marte.csv\", skiprows=10)\n",
    "\n",
    "# renombramos las columnas a algo más amigable\n",
    "station_data.columns = [\n",
    "    \"date\",\n",
    "    \"hour\",\n",
    "    \"temp\",\n",
    "    \"precip\",\n",
    "    \"humidity\",\n",
    "    \"wind_dir\",\n",
    "    \"wind_speed\",\n",
    "]\n",
    "\n",
    "# combinamos las columnas de fecha y hora en una sola\n",
    "station_data[\"date\"] = pd.to_datetime(station_data[\"date\"] + \" \" + station_data[\"hour\"])\n",
    "station_data = station_data.drop(columns=[\"hour\"])\n",
    "\n",
    "# convertimos las columnas de temperatura, precipitación, humedad y velocidad del viento a números\n",
    "station_data[\"temp\"] = pd.to_numeric(\n",
    "    station_data[\"temp\"], errors=\"coerce\", downcast=\"float\"\n",
    ")\n",
    "station_data[\"precip\"] = pd.to_numeric(\n",
    "    station_data[\"precip\"], errors=\"coerce\", downcast=\"float\"\n",
    ")\n",
    "station_data[\"humidity\"] = pd.to_numeric(\n",
    "    station_data[\"humidity\"], errors=\"coerce\", downcast=\"float\"\n",
    ")\n",
    "station_data[\"wind_dir\"] = pd.to_numeric(\n",
    "    station_data[\"wind_dir\"], errors=\"coerce\", downcast=\"float\"\n",
    ")\n",
    "station_data[\"wind_speed\"] = pd.to_numeric(\n",
    "    station_data[\"wind_speed\"], errors=\"coerce\", downcast=\"float\"\n",
    ")\n",
    "\n",
    "# eliminamos las filas con valores faltantes\n",
    "station_data = station_data.dropna()\n",
    "station_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos verificar que los datos se encuentran listos para su uso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data.describe(exclude=[np.datetime64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos generar un reporte automático de la data utilizando la librería `ydata_profiling` para tener una idea de cómo se distribuyen los datos y si existen valores faltantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ProfileReport(station_data, title=\"Campo de Marte Station Data Profiling Report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación de un modelo simple en Pytorch\n",
    "\n",
    "En esta sección implementaremos un red neuronal artificial (ANN) en Pytorch. El modelo consistirá de capas densas que tomará como entrada la temperatura, la humedad y velocidad del viento para predecir la temperatura de la hora siguiente.\n",
    "\n",
    "### Separación de los datos\n",
    "\n",
    "Antes de implementar el modelo, es necesario separar los datos en un conjunto de entrenamiento, un conjunto de validación y un conjunto de prueba. En este caso, utilizaremos el 70% de los datos para entrenar el modelo, el 15% para validar el modelo y el 15% para probar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data[\"next_temp\"] = station_data[\"temp\"].shift(-1)\n",
    "station_data = station_data.dropna()\n",
    "station_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(station_data) * 0.7)\n",
    "val_size = int(len(station_data) * 0.15)\n",
    "test_size = len(station_data) - train_size - val_size\n",
    "\n",
    "train_data = station_data[[\"temp\", \"humidity\", \"wind_speed\"]].iloc[:train_size]\n",
    "train_label = station_data[\"next_temp\"].iloc[:train_size]\n",
    "\n",
    "val_data = station_data[[\"temp\", \"humidity\", \"wind_speed\"]].iloc[\n",
    "    train_size : train_size + val_size\n",
    "]\n",
    "val_label = station_data[\"next_temp\"].iloc[train_size : train_size + val_size]\n",
    "\n",
    "test_data = station_data[[\"temp\", \"humidity\", \"wind_speed\"]].iloc[\n",
    "    train_size + val_size :\n",
    "]\n",
    "test_label = station_data[\"next_temp\"].iloc[train_size + val_size :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez separados, usamos el set de entrenamiento para estandarizar los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = train_data.mean()\n",
    "std = train_data.std()\n",
    "\n",
    "train_data_standarized = (train_data - mean) / std\n",
    "val_data_standarized = (val_data - mean) / std\n",
    "test_data_standarized = (test_data - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch proporciona la clase `Dataset` para cargar y manipular los datos. En este caso, crearemos una clase `WeatherDataset` que hereda de la clase `Dataset` y que tomará como entrada los datos y las etiquetas. Para este caso, las entradas serán la temperatura, la humedad y la velocidad del viento, y las etiquetas serán la temperatura de la hora siguiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_standarized, next_temp):\n",
    "        self.input = data_standarized[[\"temp\", \"humidity\", \"wind_speed\"]].values\n",
    "        self.output = next_temp.values\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.input.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input[idx], self.output[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de definir la clase `WeatherDataset`, crearemos tres instancias de esta clase: una para el conjunto de entrenamiento, una para el conjunto de validación y una para el conjunto de prueba. Seguidamente, crearemos tres instancias de la clase `DataLoader` que nos permitirán cargar los datos en lotes durante el entrenamiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = WeatherDataset(train_data_standarized, train_label)\n",
    "\n",
    "val_dataset = WeatherDataset(val_data_standarized, val_label)\n",
    "\n",
    "test_dataset = WeatherDataset(test_data_standarized, test_label)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=20, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=20, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición del modelo\n",
    "\n",
    "Una vez que hemos separado los datos, es necesario definir el modelo. En este caso, crearemos una clase `ANN` que hereda de la clase `Module` y que definirá la arquitectura de la red neuronal. La red neuronal consistirá en una capa densa que tomará como entrada la temperatura y la humedad y producirá como salida la velocidad del viento.\n",
    "\n",
    "La definicion de modelos en Pytorch es muy sencilla, solo necesitamos definir las capas que componen la red neuronal en el método `__init__` y definir la forma en que se propagan los datos a través de la red en el método `forward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(3, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo\n",
    "\n",
    "Esta puede ser la parte más complicada de todo el proceso, ya que Pytorch da libertad al desarrollador para definir cómo se entrena el modelo. Para usuarios avanzados, esto puede ser una ventaja, ya que permite experimentar con diferentes algoritmos de optimización y funciones de pérdida. Sin embargo, para usuarios principiantes, esto puede ser un desafío, ya que es necesario entender cómo funcionan estos algoritmos y cómo se implementan en Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instanciamos el modelo\n",
    "model = WeatherModel()\n",
    "\n",
    "# instanciamos la función de pérdida\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# instanciamos el optimizador\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# entrenamos el modelo\n",
    "epochs = 60\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = []\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target.unsqueeze(1))\n",
    "        epoch_loss.append(loss.item() * data.size(0))\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    for data, target in val_loader:\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target.unsqueeze(1))\n",
    "        val_loss.append(loss.item() * data.size(0))\n",
    "\n",
    "    # imprimimos la pérdida del epoch\n",
    "    if (epoch + 1) % 20 == 0 or epoch == 0:\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{epochs} - loss: {sum(epoch_loss) / len(train_dataset)} - val_loss: {sum(val_loss) / len(val_dataset)}\"\n",
    "        )\n",
    "    train_losses.append(sum(epoch_loss) / len(train_dataset))\n",
    "    val_losses.append(sum(val_loss) / len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(train_losses, label=\"train\")\n",
    "ax.plot(val_losses, label=\"val\")\n",
    "ax.legend()\n",
    "ax.set_title(\"Loss\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación del modelo\n",
    "\n",
    "Una vez que hemos entrenado el modelo, es necesario evaluar su rendimiento en el conjunto de validación. En este caso, utilizaremos el error cuadrático medio (MSE) como métrica de evaluación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_losses = []\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target.unsqueeze(1))\n",
    "        test_losses.append(loss.item() * len(data))\n",
    "        predictions.append(output)\n",
    "predictions = np.concatenate(predictions).ravel()\n",
    "print(f\"Test Loss: {sum(test_losses) / len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos evaluar como es que nuestra predicción se comparar con los valores reales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(test_label.values, label=\"true\")\n",
    "ax.plot(predictions, label=\"pred\")\n",
    "ax.legend()\n",
    "ax.set_title(\"Temperature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regresión lineal\n",
    "coefs = poly.polyfit(test_label.values, predictions, 1)\n",
    "ffit = poly.Polynomial(coefs)\n",
    "ffit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.scatter(test_label.values, predictions, s=5)\n",
    "ax.set_xlabel(\"True\")\n",
    "ax.set_ylabel(\"Pred\")\n",
    "\n",
    "x = np.linspace(13, 24, 100)\n",
    "y = ffit(x)\n",
    "ax.plot(x, y, color=\"red\", lw=1, label=f\"y = {coefs[0]:.2f} + {coefs[1]:.2f}x\")\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "ax.set_title(\"True vs Pred\")\n",
    "ax.set_xlim(13, 24)\n",
    "ax.set_ylim(13, 24)\n",
    "\n",
    "# linea de 45 grados\n",
    "ax.plot([13, 24], [13, 24], ls=\"--\", lw=0.5, color=\"black\")\n",
    "\n",
    "ax.grid(ls=\"--\", lw=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación de un modelo simple en Tensorflow\n",
    "\n",
    "En esta sección implementaremos un red neuronal artificial (ANN) en Tensorflow. El modelo consistirá de capas densas que tomará como entrada la temperatura, la humedad y velocidad del viento para predecir la temperatura de la hora siguiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separación de los datos\n",
    "\n",
    "Usaremos los mismos datos que utilizamos en la implementación de Pytorch, por lo que no es necesario volver a realizar este paso.\n",
    "\n",
    "### Definición del modelo\n",
    "\n",
    "Al igual que en Pytorch, es necesario definir el modelo en Tensorflow. En este caso, utilizaremos la API de alto nivel `tf.keras` para definir el modelo. La API `tf.keras` proporciona una interfaz sencilla y eficiente para definir y entrenar modelos de Deep Learning en Tensorflow. `Keras` es una librería de alto nivel que permite definir modelos de Deep Learning de forma sencilla y eficiente, por lo que es ideal para usuarios tanto principiantes como avanzados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_keras = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1),\n",
    "    ]\n",
    ")\n",
    "model_keras.build(input_shape=(None, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_keras.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de definir el modelo, es necesario compilarlo. Para compilar el modelo, es necesario definir la función de pérdida y el algoritmo de optimización. En este caso, utilizaremos el error cuadrático medio (MSE) como función de pérdida y el algoritmo de optimización Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_keras.compile(optimizer=\"adam\", loss=\"mean_squared_error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos entrenar el modelo y evaluar su rendimiento en el conjunto de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_keras.fit(\n",
    "    train_data_standarized.values,\n",
    "    train_label.values,\n",
    "    epochs=60,\n",
    "    batch_size=20,\n",
    "    validation_data=(val_data_standarized.values, val_label.values),\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos evaluar el rendimiento del modelo durante el entramiento y con los datos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = model_keras.history.history[\"loss\"]\n",
    "val_loss = model_keras.history.history[\"val_loss\"]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(train_loss, label=\"train\")\n",
    "ax.plot(val_loss, label=\"val\")\n",
    "ax.legend()\n",
    "ax.set_title(\"Loss\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_keras = model_keras.predict(test_data_standarized.values).ravel()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(test_label.values, label=\"true\")\n",
    "ax.plot(predictions_keras, label=\"pred\")\n",
    "ax.legend()\n",
    "ax.set_title(\"Temperature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs_keras = poly.polyfit(test_label.values, predictions_keras, 1)\n",
    "ffit_keras = poly.Polynomial(coefs_keras)\n",
    "ffit_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.scatter(test_label.values, predictions_keras, s=5)\n",
    "ax.set_xlabel(\"True\")\n",
    "ax.set_ylabel(\"Pred\")\n",
    "\n",
    "x = np.linspace(13, 24, 100)\n",
    "y = ffit_keras(x)\n",
    "ax.plot(\n",
    "    x, y, color=\"red\", lw=1, label=f\"y = {coefs_keras[0]:.2f} + {coefs_keras[1]:.2f}x\"\n",
    ")\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "ax.set_title(\"True vs Pred\")\n",
    "ax.set_xlim(13, 24)\n",
    "ax.set_ylim(13, 24)\n",
    "\n",
    "# linea de 45 grados\n",
    "ax.plot([13, 24], [13, 24], ls=\"--\", lw=0.5, color=\"black\")\n",
    "\n",
    "ax.grid(ls=\"--\", lw=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios\n",
    "\n",
    "Ahora que hemos implementado un modelo simple en Pytorch y Tensorflow, es hora de poner en práctica lo aprendido. A continuación se presentan una serie de ejercicios que te ayudarán a reforzar los conceptos aprendidos y a experimentar con diferentes arquitecturas de redes neuronales.\n",
    "\n",
    "1. El modelo actual trata de pronosticar la temperatura de la hora siguiente utilizando la temperatura, la humedad y la velocidad del viento como entradas. Usando Pytorch o Tensorflow, modifica el modelo para que pronostique la temperatura de 6 horas en el futuro. ¿Cómo cambia el rendimiento del modelo? Tip: Puedes utilizar la función `shift` de `pandas` para desplazar las etiquetas en el tiempo.\n",
    "\n",
    "2. La configuracion actual del modelo usa dos capas densas con 64 neuronas cada una. Experimenta con diferentes arquitecturas de redes neuronales, como agregar más capas, cambiar el número de neuronas o utilizar funciones de activación diferentes. ¿Cómo cambia el rendimiento del modelo? Tip: Las funciones de activación más comunes son `ReLU`, `Sigmoid` y `Tanh`.\n",
    "\n",
    "3. Intercambie la salida del modelo para que en lugar de predecir la temperatura de la hora siguiente, prediga la velocidad del viento. ¿Cómo cambia el rendimiento del modelo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meteo_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
